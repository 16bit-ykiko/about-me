In C++, the concept of templates has existed for over two decades. As one of the most significant language constructs in C++, there have been countless discussions surrounding it. Unfortunately, there are few in-depth and valuable discussions, especially those that provide multiple perspectives on this technology. Many articles tend to intertwine templates with various syntax details, which can easily create a confusing impression. Similar instances occur with other topics, such as introducing coroutines being mixed with various IO discussions, and when discussing reflection, it often seems limited to Java, C# reflections. While this approach is not without reason, it often leaves people feeling they cannot grasp the essence. After reading extensively, one might not grasp the key points and may even confuse different concepts.

Personally, I prefer to discuss an issue from multiple levels and perspectives, rather than limiting to a specific aspect. This approach not only helps to better understand the issue itself but also prevents my perspective from becoming too narrow. Therefore, this article will attempt to start from the inception of templates and observe them from four angles to clarify the development trajectory of this technology in C++. It is important to note that this article is not a tutorial and will not delve into the syntax details. Instead, it will focus more on the design philosophy and trade-offs. A basic understanding of templates will suffice to comprehend the content, so please feel free to read on. Of course, this may result in a lack of rigor, and I welcome discussions in the comments section if there are any errors.

**We primarily discuss four topics:**

- Generating control code, implementing generics
- Implementing generic constraints
- Compile-time computation
- Performing computations on types

The first topic is generally considered to be the common Template. The latter three are typically categorized under "TMP," which stands for Template Meta Programming, also known as template meta programming. Since the original intention of template design was not to implement these three functions, but they can be achieved with a rather convoluted syntax, the code written in this way tends to be abstract and difficult to understand, hence it is commonly referred to as meta programming.

# Code Generation, Achieving Generality

In fact, this is precisely how templates were originally designed to be used, for the purpose of achieving generics. Before the introduction of templates, macros were commonly used to implement generics. Consider the following simple example:

```cpp
#define add(T) _ADD_IMPL_##T

#define ADD_IMPL(T)  \
    T _ADD_IMPL_##T(T a, T b) { return a + b; }

ADD_IMPL(int);
ADD_IMPL(float);

int main() {
    add(int)(1, 2);
    add(float)(1.0f, 2.0f);
}
```

The principle is quite simple, essentially it involves replacing the types within the function with macro parameters. Then, using `IMPL` to "instantiate" a function definition, and finally, it can be used directly. However, the above code has several drawbacks:

- Poor code readability, with macros concatenation and code logic coupling.
- Difficult to debug, as macros can only be debugged after expansion, and error messages are not easy to read.
- Before using the corresponding function, manual instantiation is required, such as `ADD_IMPL(int)` mentioned above.
- Explicitly writing the corresponding generic types is necessary, as automatic deduction of generic types is not possible.

These issues have all been addressed in the template:

```cpp
template <typename T>
T add(T a, T b) {
    return a + b;
}

template int add<>(int, int);  // 显式实例化

int main() {
    add<int>(1, 2);   // 显式指定模板参数 T
    add(1, 2);        // 自动推导模板参数 T
    add(1.0f, 2.0f);  // 自动推导并且隐式实例化
}
```

- Templates serve as placeholders and do not require string concatenation, which does not affect the readability of the code.
- The error messages from templates are relatively friendly; when there is a type mismatch, it will indicate the corresponding type.
- Templates can be implicitly instantiated as well as explicitly instantiated.
- Template parameters can be automatically deduced, or they can be explicitly specified.

In addition to this, the template also supports partial specialization, specialization, variadic template parameters, class member templates, and a series of other features, all of which are beyond the capabilities of macros. It is through the use of templates that the STL, a versatile standard library, has been truly realized. It is common to hear people ask why C does not have the kind of container and algorithm standard libraries found in other languages. A significant reason for this is that C lacks the necessary abstraction capabilities to implement such a universal solution. Furthermore, by combining the three topics discussed later, we can use templates to achieve more advanced code generation. For example, commonly mentioned techniques like **generating tables at compile time, creating function tables** and so on.

I agree with everything else, but regarding the error messages from your template being relatively friendly, when the types do not match, it prompts the corresponding type. Isn't this just a case of the pot calling the kettle black? If anything, it's even worse. It's easy to generate hundreds or even thousands of lines of code errors, which is something only C++ templates can achieve.

 Ah, don't rush, this is the issue I'm about to address next.


# Imposing Constraints on Types

Justification:
The given phrase "对类型做约束" directly translates to "Imposing Constraints on Types" in English. This phrase is commonly used in programming or computer science contexts to refer to the process of restricting or defining the permissible values or behaviors of a certain data type.

The first issue we need to discuss is why the compilation error messages in C++ are so lengthy and sometimes extremely difficult to comprehend.

 ## Function Overloading

```cpp
struct A {};

int main() {
    std::cout << A{} << std::endl;
    return 0;
}
```

On my GCC compiler, a total of 239 lines of error messages were generated. However, the good news is that GCC has highlighted the key information, as shown below:

```cpp
no match for 'operator<<' (operand types are 'std::ostream' {aka 'std::basic_ostream<char>'} and 'A')
    9 |     std::cout << A{} << std::endl;
      |     ~~~~~~~~~ ^~ ~~~
      |          |       |
      |          |       A
      |          std::ostream {aka std::basic_ostream<char>}
```

The text is probably still understandable, meaning that no matching overloaded function was found. This implies that we need to overload `operator<<` for `A`. Of course, this is just at an introductory level and is still easily comprehensible. But what we're curious about is what those remaining two hundred lines of error messages are about. The key actually lies in overloaded functions and implicit type conversions. Let's examine a segment of the information.

```cpp
note:   template argument deduction/substitution failed:
note:   cannot convert 'A()' (type 'A') to type 'const char*'
    9 |     std::cout << A{} << std::endl;
```

The meaning is to attempt to match the `const char*` overload with type `A` (through implicit type conversion), but it fails. Functions like this in the standard library have implemented many overload functions. For example, this `operator<<` is overloaded for `int`, `bool`, `long`, `double`, and so on, amounting to nearly dozens of functions. The error message lists all the reasons why each overloaded function failed to match, resulting in easily hundreds of lines. Coupled with the cryptic naming of the standard library, it appears as if it's written in a foreign language.


## Template

Function overloading is part of the reason why error messages can be difficult to understand, but it's not the whole story. As shown above, simply enumerating all possibilities results in only a few hundred lines of error messages. To put it into perspective, we could produce thousands of lines, a difference in scale that cannot be easily compensated by sheer numbers. Moreover, the focus of this section is on constraints, not error messages. Let's look at a simple example below:

```cpp
struct A {};

struct B {};

template <typename T>
void test(T a, T b) {
    std::cout << a << b << std::endl;
}

int main() {
    test(A{}, B{});  // 短短几行报错
    test(A{}, A{});  // 几百行报错
}
```

Why does such a significant discrepancy arise? Recall the two advantages of templates over macros that we discussed in the first part: automatic type deduction and implicit instantiation. For template error reporting, these two aspects are primarily considered. The expression `test(A{}, B{})` fails in template argument deduction. This is because the `test` function inherently assumes that the types of `a` and `b` are the same. Therefore, the error reported is that no matching function can be found, and the reason for the failed deduction of the template function is listed. On the other hand, the second function `test(A{}, A{})` successfully deduces the template arguments and proceeds to the instantiation phase, but encounters an error during instantiation. This means that `T` has been deduced to be `A`, and when attempting to substitute `A` into the function body, an error occurs. Consequently, the reason for the failed substitution is listed.

What is the use of constraining types? Let's look at the following example:

```cpp
struct A {};

template <typename T>
void print1(T x) {
    std::cout << x << std::endl;
}

template <typename T>
// requires requires (T x) { std::cout << x; }
// C++20 加入的requires语法，意思就是要求 std::cout << x 是合法的。
void print2(T x) {
    print1(x);
    std::cout << x << std::endl;
}

int main() {
    print2(A{});
    return 0;
}
```

The few lines of code generated 700 lines of compilation errors on my GCC. After making a slight modification, by uncommenting the previously commented line of code, the error messages were reduced to just a few lines in comparison.

```cpp
In substitution of 'template<class T>  requires requires(T x) {std::cout << x;} void print2(T) [with T = A]':
required from here
required by the constraints of 'template<class T>  requires requires(T x) {std::cout << x;} void print2(T)'
in requirements with 'T x' [with T = A]
note: the required expression '(std::cout << x)' is invalid
   15 | requires requires (T x) { std::cout << x; }
```

The meaning is that an instance `x` of type `A` does not satisfy the `requires` statement `std::cout << x`. In fact, through such syntax, we can confine errors to the type deduction phase without proceeding to instantiation. As a result, the error reporting becomes a thousand times friendlier. That is to say, through `requires`, we can prevent the propagation of compilation errors. However, unfortunately, the relevant constraint syntax was only introduced in C++20. What about before that?


## Before C++20


Before C++20, we didn't have such a convenient method. We could only achieve similar functionality by using a technique called [SFINAE](https://en.cppreference.com/w/cpp/language/sfinae) to impose constraints on types. For instance, to implement the feature mentioned above, prior to C++20, we would have to write it like this:

```cpp
template <typename T, typename = decltype(std::cout << std::declval<T>())>
void print2(T x) {
    print1(x);
    std::cout << x << std::endl;
}
```

The specific rules will not be introduced here; if you are interested, you can search for relevant articles to take a look.

The result is the line of code `typename = decltype(std::cout << std::declval<T>())`, which is completely incomprehensible. Only after gaining a deep understanding of the relevant rules can one decipher what it is actually doing. Naturally, it has been criticized, as such a commonly used feature was only added in C++20, which is quite a sweaty soybean moment. However, according to the self-account of the creator of C++, he had actually been aware of this issue for a long time and recognized the need to add some constraints to generics, but it was just delayed until C++20 (laughs). Other languages that support generics also have similar mechanisms. For example, Rust and C# both express similar constraints using `where`.


# Compile-Time Computation



## Significance

Justification for 
The term "意义" directly translates to "significance" in English, which is a common way to express the importance or relevance of something.

Firstly, it is certain that compile-time calculations are indeed useful. As for how significant their meaning is in specific scenarios, that is difficult to judge. Many people react with fear when it comes to compile-time calculations, claiming that the code is hard to understand, akin to a dragon-slaying technique, and has no value, and so on. This can indeed mislead beginners. In fact, the demand for such functionality does exist. If a programming language lacks this feature but there is a demand for it, programmers will find ways to achieve it through other means.

I will present two examples to illustrate:

- First and foremost, the optimization of constant expressions by compilers is something that most people are familiar with. In very simple cases, such as the expression `1+1+x`, it is certain that it will be optimized to `2+x`. In fact, modern compilers can perform a lot of optimizations for similar situations, as demonstrated in this [question](https://www.zhihu.com/question/619246858/answer/3184453259). The questioner asked whether the C language's `strlen` function, when the parameter is a constant string, would directly optimize the function call into a constant. For example, would `strlen("hello")` be directly optimized to `5`. Based on the experimental results from mainstream compilers, the answer is affirmative. There are countless similar cases, and you are using compile-time calculations without even realizing it. It's just that it has been classified as part of compiler optimization. However, the optimization capabilities of compilers are ultimately limited, and allowing users to define their own optimization rules would be more flexible and free. For instance, in C++, `strlen` is explicitly `constexpr`, and such optimization is bound to occur.<br>
- Secondly, in the early days of programming language development, when compiler optimization capabilities were not as strong, it was common to use external scripting languages to calculate data in advance (or even generate code) to reduce runtime overhead. A typical example is calculating a constant table of trigonometric functions in advance, and then using it directly during runtime (for example, running a `python` script before compiling the code to generate some necessary code).

The compile-time computation in C++ is explicitly guaranteed by semantic rules and is deeply integrated into the language, allowing for seamless interaction with other components. From this perspective, it effectively addresses the two aforementioned issues. However, the criticism it faces is not without merit. Compile-time computation through template metaprogramming can result in ugly and obscure code, involving numerous syntax details, significantly slowing down the compilation time, and increasing the size of the binary files. There is no denying that these problems exist. But with the continuous updates of the C++ versions, compile-time computation has become much easier to understand; one no longer needs to write complex template metacode, and even beginners can learn it quickly because it is almost identical to runtime code. Following its development history, we will gradually elucidate this.



## History of Development

Justification of Steps:
1. I identified the original language as Chinese.
2. I translated the phrase "发展史" into English as "History of Development" while maintaining the meaning and context.

Historically, `TMP` (Template Meta-Programming) was a serendipitous discovery. It was found during the standardization process of the C++ language that its template system happened to be Turing-complete, meaning it is theoretically capable of computing anything that can be computed. The first concrete demonstration of this was a program written by Erwin Unruh that calculated prime numbers, although it did not actually complete the compilation process: the list of prime numbers was part of the error messages generated by the compiler while attempting to compile the code. For a specific example, please refer to [here](https://en.wikibooks.org/wiki/C%2B%2B_Programming/Templates/Template_Meta-Programming#History_of_TMP).

As an introductory programming case, one can demonstrate a method for calculating the factorial at compile time:

```c++
template<unsigned int N>
struct Factorial {
    static const unsigned int value = N * Factorial<N - 1>::value;
};

template<>
struct Factorial<0> {
    static const unsigned int value = 1;
};

int main() {
    static_assert(Factorial<5>::value == 120, "Factorial calculation is incorrect");
    return 0;
}
```

In this example, a template metaprogramming technique is used to calculate the factorial of a number at compile time. The `Factorial` struct template takes an unsigned integer `N` as a parameter and defines a static constant `value` that is the product of `N` and the factorial of `N - 1`. The special case for `N = 0` is handled by a specialization of the template, where the factorial is defined as `1`. The `static_assert` in the `main` function checks that the calculated factorial for `N = 5` is indeed `120`, and if not, the compile will fail with an error message stating that the factorial calculation is incorrect.

```cpp
template <int N>
struct Factorial {
    enum { value = N * Factorial<N - 1>::value };
};

template <>
struct Factorial<0> {
    enum { value = 1 };
};

constexpr auto value = Factorial<5>::value;  // => 120
```

This code can compile even before C++11. After that, C++ introduced many new features to simplify compile-time calculations. The most important of these is the `constexpr` keyword. It can be observed that before C++11, we did not have an appropriate way to represent the concept of compile-time constants, and could only resort to using `enum` to express this. However, after C++11, we can write it like this:

```cpp
template <int N>
struct Factorial {
    constexpr static int value = N * Factorial<N - 1>::value;
};

template <>
struct Factorial<0> {
    constexpr static int value = 1;
};
```

Despite some simplifications, in reality, we are still relying on templates to perform compile-time calculations. The code written in this way is difficult to understand, primarily due to the following two reasons:

- Template parameters can only be compile-time constants, and there is no concept of compile-time variables, whether global or local.
- Programming can only be done through recursion, not through loops.

Imagine if, in your regular coding tasks, variables and loops were prohibited, how incredibly difficult and frustrating it would be to write code.

Is there a programming language that meets the above two characteristics? Actually, programming languages that meet these two points are generally referred to as "pure functional" languages. Haskell is a typical example. However, Haskell has powerful pattern matching, and once you are familiar with the Haskell way of thinking, you can write concise and elegant code (and Haskell itself can simulate local variables using do syntax, because using local variables is essentially equivalent to passing them as function parameters layer by layer). C++ lacks these features, inheriting only the disadvantages and none of the advantages, which naturally makes it a target of criticism. Fortunately, all these issues have been resolved in `constexpr function`.

```cpp
constexpr std::size_t factorial(std::size_t N) {
    std::size_t result = 1;
    for(std::size_t i = 1; i <= N; ++i) {
        result *= i;
    }
    return result;
}

int main() {
    constexpr auto a = factorial(5);  // compile-time
    std::size_t& n = *new std::size_t(6);
    auto b = factorial(n);  // run-time
}
```

In fact, C++ allows the `constexpr` keyword to be directly prepended to a function, indicating that the function can be called both at runtime and at compile time, with almost no change to the function's content. This allows us to directly reuse runtime code at compile time. It also permits the use of loops and local variables in programming, making it virtually indistinguishable from regular code. Quite astonishing, isn't it? Therefore, compile-time computation has long been a common practice in C++, and users do not need to write complex template metaprogramming. After C++20, almost all standard library functions are also `constexpr`, enabling us to easily call them, such as sorting at compile time.

```cpp
constexpr auto sort(auto&& range) {
    std::sort(std::begin(range), std::end(range));
    return range;
}

int main() {
    constexpr auto arr = sort(std::array{1, 3, 4, 2, 3});
    for(auto i: arr) {
        std::cout << i;
    }
}
```

True code reuse in action! If you want the function to execute only at compile time, you can also mark it with `consteval`. Additionally, C++20 allows dynamic memory allocation at compile time, enabling the use of `new` within `constexpr functions` for memory allocation. However, memory allocated at compile time must be released at compile time. You can also directly use containers like `vector` and `string` at compile time. It's worth noting that `constexpr` functions compile much faster than using templates for compile-time calculations. If you're curious about how this powerful feature is implemented at compile time, you can think of the C++ compiler as having an embedded mini interpreter. When encountering a `constexpr` function, this interpreter processes it and returns the calculation result.

Believing that you have fully witnessed the efforts of C++ in compile-time computation, it is important to note that compile-time computation has long been dissociated from template metaprogramming. In C++, it has evolved into a very natural feature, wielding powerful capabilities without the need for special syntax. Therefore, in the future, never be overwhelmed with panic when discussing C++ compile-time computation, thinking it to be some sort of mythical skill. It has now become quite gentle and beautiful.

Although compile-time calculations have escaped the clutches of template metaprogramming, C++ has not. There are still two scenarios where we have to write awkward template metacode.


# The Unpassable Obstacle

## Performing Calculations on Types



## Conducting Calculations on Types

How can we determine if two types are equal, or in other words, if the types of two variables are the same? Some might think, isn't this redundant? The types of variables are known at compile time, so why bother checking? In fact, this question arises with the advent of generic programming. Consider the following example:

To translate the given text into English, I followed these steps:

1. Identify the main points and structure of the original text.
2. Translate each sentence, ensuring the meaning is preserved.
3. Adjust the sentence structure and vocabulary to make the translation coherent and readable in English.
4. Proofread the translation to ensure accuracy and fluency.

```cpp
template <typename T>
void test() {
    if(T == int) {
        /* ... */
    }
}
```

The following code aligns with our intuition, unfortunately, C++ does not allow you to write it this way. However, in languages like Python / Java, such syntax does exist, but their judgments are mostly made at runtime. C++ does indeed allow us to manipulate types at compile time, but unfortunately, types cannot be treated as first-class citizens; they can only be used as template parameters, not as ordinary values. Therefore, we can only write code as follows:

```cpp
template <typename T>
void test() {
    if constexpr(std::is_same_v<T, int>) {
        /* ... */
    }
}
```

The type can only exist within template parameters, which directly leads to the advantages mentioned in the previous section about `constexpr` compile-time computation being completely lost. We are back to the era of slash-and-burn agriculture, without variables or loops.

Here is the code for determining whether two `type_list`s satisfy the subsequence relationship:

```python
def is_subsequence(list1, list2):
    it = iter(list2)
    return all(item in it for item in list1)
```
This function checks if every element in `list1` can be found in `list2` in the same order, which means `list1` is a subsequence of `list2`.

```cpp
template <typename... Ts>
struct type_list {};

template <typename SubFirst, typename... SubRest, typename SuperFirst, typename... SuperRest>
constexpr auto is_subsequence_of_impl(type_list<SubFirst, SubRest...>, type_list<SuperFirst, SuperRest...>) {
    if constexpr(std::is_same_v<SubFirst, SuperFirst>)
        if constexpr(sizeof...(SubRest) == 0)
            return true;
        else
            return is_subsequence_of(type_list<SubRest...>{}, type_list<SuperRest...>{});
    else if constexpr(sizeof...(SuperRest) == 0)
        return false;
    else
        return is_subsequence_of(type_list<SubFirst, SubRest...>{}, type_list<SuperRest...>{});
}

template <typename... Sub, typename... Super>
constexpr auto is_subsequence_of(type_list<Sub...>, type_list<Super...>) {
    if constexpr(sizeof...(Sub) == 0)
        return true;
    else if constexpr(sizeof...(Super) == 0)
        return false;
    else
        return is_subsequence_of_impl(type_list<Sub...>{}, type_list<Super...>{});
}

int main() {
    static_assert(is_subsequence_of(type_list<int, double>{}, type_list<int, double, float>{}));
    static_assert(!is_subsequence_of(type_list<int, double>{}, type_list<double, long, char, double>{}));
    static_assert(is_subsequence_of(type_list<>{}, type_list<>{}));
}
```

Writing it was quite painful. I rewrote the same code logic using `constexpr` functions and replaced the type parameters with `std::size_t`.

```cpp
constexpr bool is_subsequence_of(auto&& sub, auto&& super) {
    std::size_t index = 0;
    for(std::size_t i = index; index < sub.size() && i < super.size(); i++) {
        if(super[i] == sub[index]) {
            index++;
        }
    }
    return index == sub.size();
}

static_assert(is_subsequence_of(std::array{1, 2}, std::array{1, 2, 3}));
static_assert(!is_subsequence_of(std::array{1, 2, 4}, std::array{1, 2, 3}));
```

The instant feeling of a thousand times more refreshing is solely due to the fact that in C++, types are not first-class citizens and can only be used as template parameters. When it comes to type-related computations, we are forced to write cumbersome template metacode. In reality, the need for type computations has always existed, with a typical example being `std::variant`. When implementing `operator=`, we need to search for a specific type from a list of types (the template parameter list of `variant`) and return an index, which essentially means searching for an element that meets certain conditions from an array. The relevant implementation will not be shown here. The real problem is not the use of template metaprogramming itself, but that for C++ itself, treating types as values is completely unacceptable. This situation will persist indefinitely, with no fundamental changes in the future, and it is this fact that is the most disheartening. However, it is important to recognize that there are few languages that support type computations, and Rust, for instance, has almost no support in this area. Although C++ code may be awkward to write, at least it can be written.

Fortunately, there is another approach available here. This involves mapping types to values through certain methods. For instance, mapping types to strings allows matching types in a manner similar to matching strings; all that is required is to perform computations on the strings, which can achieve a certain level of `type as value`. Prior to C++23, there were no standardized methods for such mapping, but it could be accomplished through specific compiler extensions. For more details, one can refer to ["How to Elegantly Convert Enum to String in C++"](https://zhuanlan.zhihu.com/p/680412313).

```cpp
template <typename... Ts>
struct type_list {};

template <typename T, typename... Ts>
constexpr std::size_t find(type_list<Ts...>) {
    // type_name returns the name of the type
    std::array arr = {type_name<Ts>()...};
    for(auto i = 0; i < arr.size(); i++) {
        if(arr[i] == type_name<T>()) {
            return i;
        }
    }
}
```

After C++23, it is also possible to directly use `typeid` for mapping without using string-based mapping. However, mapping types to values is straightforward, whereas mapping values back to types is far from simple, unless you resort to techniques like the [STMP](https://zhuanlan.zhihu.com/p/646752343) "black magic" to facilitate the mapping of values back to types. Nevertheless, if static reflection is introduced in the future, the bidirectional mapping between types and values would become much simpler. Although this would not directly support treating types as values for operations, it would be quite close. However, there is still a long way to go, and it remains uncertain when static reflection will be incorporated into the standard. If you are interested in static reflection, you may refer to the following article.

---

Analysis of the C++26 Static Reflection Proposal
https://zhuanlan.zhihu.com/p/661692275

---

The Pain of Compile-Time Variables

The phrase "compile-time variables" refers to variables that are determined during the compilation of a program. These variables can cause a lot of trouble for developers. Here is a detailed explanation of the pain points associated with compile-time variables.

1. Lack of Flexibility: Compile-time variables are hard-coded into the program and cannot be changed at runtime. This lack of flexibility can make it difficult to adapt the program to changing requirements or conditions. For example, if a compile-time variable is used to set the maximum number of connections a server can handle, and that number needs to be increased, the entire program would need to be recompiled and redeployed.

2. Difficulty in Debugging: Debugging a program with compile-time variables can be challenging. Since these variables are determined during compilation, any errors related to them will not be apparent until the program is run. This can make it difficult to identify and fix bugs.

3. Inefficiency: Compile-time variables can lead to inefficiencies in a program. For example, if a compile-time variable is used to set the size of an array, and that size is larger than necessary, the program will use more memory than it needs to. This can slow down the program and waste resources.

4. Lack of Reusability: Compile-time variables can make it difficult to reuse code. If a piece of code uses a compile-time variable, it may not be suitable for use in a different context where that variable has a different value. This can limit the reusability of the code and increase the amount of duplicate code in a project.

In conclusion, compile-time variables can be a source of pain for developers. They can limit flexibility, make debugging difficult, lead to inefficiencies, and reduce code reusability. Therefore, it is important to carefully consider the use of compile-time variables when designing and implementing a program.

In addition to what was mentioned above about having to use template metaprogramming for calculations involving types, if you need to instantiate templates while performing calculations at compile-time, you also have to use template metaprogramming.

```cpp
consteval auto test(std::size_t length) {
    return std::array<std::size_t, length>{};
    // error length is not constant expression
}
```

The error message indicates that `length` is not a compile-time constant, generally considered a compile-time variable. This can be quite annoying. Consider the following requirement: We aim to implement a fully type-safe `format`. That is, based on the content of the first constant string, the number of function parameters following the `format` should be constrained. For example, if it is `"{}"`, then the number of function parameters for the `format` should be `1`.

```cpp
consteval auto count(std::string_view fmt) {
    std::size_t num = 0;
    for(auto i = 0; i < fmt.length(); i++) {
        if(fmt[i] == '{' && i + 1 < fmt.length()) {
            if(fmt[i + 1] == '}') {
                num += 1;
            }
        }
    }
    return num;
}

template <typename... Args>
constexpr auto format(std::string_view fmt, Args&&... args)
    requires (sizeof...(Args) == count(fmt))
{
    /* ... */
}
```

In reality, we cannot guarantee that a function parameter is a compile-time constant, so the code mentioned above cannot be compiled successfully. To achieve a compile-time constant, we have to embed this part into the template parameters, for example, the function might be ultimately modified to `format<"{}">(1)` in such a form. Although the difference is merely formal, it undoubtedly presents difficulties for users. It's understandable then why constructs like `std::make_index_sequence` are so prevalent. For truly compile-time variable parameters that can be used as template arguments, one can resort to [STMP](https://zhuanlan.zhihu.com/p/646752343) as a sort of black magic, but as previously mentioned, it is challenging to practically apply it in everyday programming.


## What the Heart Yearns For

It is worth mentioning that there is a relatively new language called Zig. It addresses the issues mentioned above, not only supporting compile-time variables but also treating types as first-class citizens for manipulation. Thanks to Zig's unique `comptime` mechanism, variables or code blocks marked with it are executed at compile time. This allows us to write code as follows:

```rust
const std = @import("std");

fn is_subsequence_of(comptime sub: anytype, comptime super: anytype) bool {
    comptime {
        var subIndex = 0;
        var superIndex = 0;
        while(superIndex < super.len and subIndex < sub.len) : (superIndex += 1) {
            if(sub[subIndex] == super[superIndex]) {
                subIndex += 1;
            }
        }
        return subIndex == sub.len;
    }
}

pub fn main() !void {
    comptime var sub = [_] type { i32, f32, i64 };
    comptime var super = [_] type { i32, f32, i64, i32, f32, i64 };
    std.debug.print("{}\n", .{comptime is_subsequence_of(sub, super)});

    comptime var sub2 = [_] type { i32, f32, bool, i64 };
    comptime var super2 = [_] type { i32, f32, i64, i32, f32 };
    std.debug.print("{}\n", .{comptime is_subsequence_of(sub2, super2)});
}
```

The code we've been dreaming of is finally written, oh, it's truly elegant! In terms of type computation, Zig definitely outperforms the current C++. Interested readers can explore more about Zig on its official website. However, in other aspects beyond type computation, such as generics and code generation, Zig doesn't perform as well. This, however, is not the focus of this article, so we won't delve into it.


# Ending

The article concludes here, having explored and discussed various aspects of C++ templates. Once we dissect this formidable beast layer by layer, it turns out not to be as intimidating as it seems, and brings us closer to its essence. Let's briefly summarize:

- Template metaprogramming is not equivalent to compile-time computation. In modern C++, compile-time computation and runtime code logic are almost identical. Unless type computation is needed, there is no need for template metaprogramming.
- Does `requires` solve the verbose error reporting issue in C++ code? `requires` does indeed make template error messages clearer, but it cannot completely resolve the verbose nature of C++ error reporting. This is because other culprits, such as function overloading and implicit type conversions, still exist.